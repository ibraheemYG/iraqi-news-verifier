import os
from typing import List, Dict

import ollama
try:
    import google.generativeai as genai
    print("โ google.generativeai imported successfully")
except Exception as e:
    genai = None
    print(f"โ Failed to import google.generativeai: {e}")

# Always prioritize environment variables for API keys
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    try:
        from config import GEMINI_API_KEY as CONFIG_KEY
        GEMINI_API_KEY = CONFIG_KEY
    except Exception:
        pass

# Debug: Check if key is loaded (show only first/last 4 chars for security)
if GEMINI_API_KEY:
    key_preview = f"{GEMINI_API_KEY[:4]}...{GEMINI_API_KEY[-4:]}" if len(GEMINI_API_KEY) > 8 else "***"
    print(f"โ GEMINI_API_KEY loaded: {key_preview}")
else:
    print("โ GEMINI_API_KEY not found!")

def build_context_block(retrieved_context: List[Dict]) -> str:
    blocks = []
    for ctx in retrieved_context:
        blocks.append(
            f"ุงููุตุฏุฑ: {ctx['url']}\nุงูุนููุงู: {ctx['title']}\nุงููุญุชูู ุงููุฎุชุตุฑ: {ctx['body']}\nุฏุฑุฌุฉ ุงูุชุดุงุจู: {ctx.get('similarity', 0):.2f}"
        )
    return "\n\n".join(blocks)


def generate_response(user_query: str, retrieved_context: List[Dict], is_relevant: bool) -> tuple[str, bool]:
    """
    Generates a response using Gemini based on the user query and retrieved context.
    Returns the response string and a boolean indicating if the query was a question.
    """
    # --- Intent Detection: Is the user asking a question? ---
    question_words = ["ูู", "ูุงุฐุง", "ูุชู", "ุฃูู", "ููุงุฐุง", "ููู", "ูู", "ุจูู", "ูู"]
    # A simple check to see if any of the question words are in the query
    is_question = any(word in user_query.strip().split() for word in question_words)

    # --- Prompt Engineering ---
    kb = build_context_block(retrieved_context)

    if is_question:
        # Prompt for answering questions based on provided context
        prompt = f"""
        ุฃูุช ูุณุงุนุฏ ุจุงุญุซ ูุชุฎุตุต ูู ุงูุดุฃู ุงูุนุฑุงูู. ุงูุฑุฃ ููุท ุงููุตูุต ุงูุชุงููุฉ ูุฃุฌุจ ุนู ุณุคุงู ุงููุณุชุฎุฏู ุงุนุชูุงุฏุงู ุนูููุง ููุท.
        ---
        {kb}
        ---
        ุณุคุงู ุงููุณุชุฎุฏู: "{user_query}"

        ุฅุฑุดุงุฏุงุช ุตุงุฑูุฉ:
        1) ุงุณุชุฎุฏู ุงููุนูููุงุช ุงูููุฌูุฏุฉ ููุท ูู ุงููุตูุต ุฃุนูุงู. ูุง ุชุถู ุฃู ูุนูููุงุช ุฎุงุฑุฌูุฉ.
        2) ุฅุฐุง ูุงูุช ุงููุตูุต ุชุญุชูู ุตุฑุงุญุฉู ุนูู ุฅุฌุงุจุฉุ ุงุจุฏุฃ ุงูุณุทุฑ ุงูุฃูู ุจุฌููุฉ ููุฌุฒุฉ ุซู ูุฏูู ุฌููุฉ ูุงุญุฏุฉ ุชุดุฑุญ ุฃู ูุต ูุฏุนู ุงูุฅุฌุงุจุฉ ูุงุฐูุฑ ุงูุฑุงุจุท ุงูุฃูุซุฑ ุตูุฉ.
        3) ุฅุฐุง ูู ุชุญุชูู ุงููุตูุต ุนูู ุฅุฌุงุจุฉ ูุงููุฉุ ุงูุชุจ ุจุฏูุฉ: "ูุง ุชูุฌุฏ ูุนูููุงุช ูุงููุฉ ูู ุงููุตุงุฏุฑ ููุฅุฌุงุจุฉ ุนูู ูุฐุง ุงูุณุคุงู." ุซู ุงูุชุฑุญ ูุตุฏุฑูุง ุฃู ูุตุทูุญ ุจุญุซ ููุญุณูู ุงููุชุงุฆุฌ.
        4) ูู ููุฌุฒูุง (ุณุทุฑ ุฅูู ุณุทุฑูู ุฅุถุงูููู ูุญุฏ ุฃูุตู). ุงููุบุฉ: ุงูุนุฑุจูุฉ.
        """
    elif is_relevant:
        # Prompt for verifying a news claim that has relevant context
        prompt = f"""
        ุฃูุช ูุธุงู ุชุญูู ุฃุฎุจุงุฑ ูุชุญูุธ ููุญุงูุฏ ูุฐูู. ูุฏูู ูุฐู ุงููุตูุต ููุท:
        ---
        {kb}
        ---
        ุงุฏุนุงุก ุงููุณุชุฎุฏู: "{user_query}"

        ุฅุฑุดุงุฏุงุช ุตุงุฑูุฉ ููุชุงุจุฉ ุงูุญูู:
        1) ุงุณุชูุฏ ููุท ุฅูู ุงููุตูุต ุงููุนุทุงุฉ. ูุง ุชุถู ุฃู ุชุฎูู ูุนูููุงุช ุฎุงุฑุฌูุฉ.
        2) ูู ุฐููุงู: ูุฏ ูุณุชุฎุฏู ุงููุณุชุฎุฏู ุงุฎุชุตุงุฑุงุช ุฃู ุฃุณูุงุก ูุฎุชููุฉ (ูุซู "ููุชู" ูู"ููููุณููุณ"ุ "ุงูุฑูุงู" ูู"ุฑูุงู ูุฏุฑูุฏ"ุ "ุฌุฒุงุก" ูู"ุฑููุฉ ุฌุฒุงุก"). ุฅุฐุง ูุฌุฏุช ุชุทุงุจูุงู ูู ุงููุนูู ูุน ุงูุฃุณูุงุก ุงููุฎุชููุฉุ ุงุนุชุจุฑู ุชุทุงุจูุงู ุตุญูุญุงู.
        3) ุงุจุฏุฃ ุงูุณุทุฑ ุงูุฃูู ุฅูุง ุจู "โ ุงูุฎุจุฑ ููุซูู" ุฃู "โ๏ธ ุงูุฎุจุฑ ุบูุฑ ูุคูุฏ" ุจุญูุซ ูุชูุงูู ูุฐุง ุงูุนููุงู ุจุฏูุฉ ูุน ุงูุดุฑุญ ุงูุชูุตููู ุงูุฐู ููู.
        4) ุฅุฐุง ุฃูุฏุช (โ): ุงุฐูุฑ ูู ุณุทุฑ ูุงุญุฏ ุฃู ุฌููุฉ/ููุทุน ูู ุงููุตูุต ูุฏุนู ุงูุงุฏุนุงุก ูููุงุฐุง. ุฅุฐุง ูุงู ุงููุณุชุฎุฏู ุงุณุชุฎุฏู ุงุณูุงู ูุฎุชุตุฑุงูุ ุงุฐูุฑ ุฐูู ุจุดูู ูุงุถุญ (ูุซุงู: "ููุชู ูู ุงูุงุณู ุงููุฎุชุตุฑ ูููููุณููุณ").
        5) ุฅุฐุง ูู ุชุชููู ูู ุงูุชุฃููุฏ (โ๏ธ): ุงุดุฑุญ ุจุฅูุฌุงุฒ ุณุจุจ ุนุฏู ุงูุชุฃููุฏ (ุชุถุงุฑุจ ูู ุงููุตูุตุ ุบูุงุจ ุงูุชูุงุตููุ ูุฌุฑุฏ ุชููู).
        6) ุทูู ุงูุฅุฌุงุจุฉ: ุฅุฌูุงูู 2-4 ุฌูู ุจุนุฏ ุงูุณุทุฑ ุงูุฃูู. ูุง ุชุชูุฑุฑ.
        7) ุงููุบุฉ: ุงูุนุฑุจูุฉ. ุงูุชุฒู ุจุงูุตูุงุบุฉ ูุงููุจุฑุฉ ุงูููููุฉ ูุงููุญุงูุฏุฉ.
        """
    else:
        # Prompt for a claim with no relevant context found
        prompt = f"""
        ุฃูุช ูุธุงู ุชุญูู ุฃุฎุจุงุฑ. ููุฏ ุจุญุซุช ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช ููู ุชุฌุฏ ุฃู ูุตูุต ุฐุงุช ุตูุฉ.
        ุงุฏุนุงุก ุงููุณุชุฎุฏู: "{user_query}"

        ุฅุฑุดุงุฏุงุช:
        1) ุงุจุฏุฃ ุงูุณุทุฑ ุงูุฃูู ุจู "โ๏ธ ุงูุฎุจุฑ ุบูุฑ ูุคูุฏ".
        2) ุงุดุฑุญ ุจุงุฎุชุตุงุฑ (1-2 ุฌูู) ุฃู ูุงุนุฏุฉ ุงููุตุงุฏุฑ ูุง ุชุญุชูู ุนูู ูุนูููุงุช ุชุฏุนู ุงูุงุฏุนุงุกุ ูุงุฐูุฑ ุฃูู ุงุณุชูุฏุช ููุท ุฅูู ุงููุตูุต ุงูููุฌูุฏุฉ.
        3) ุงูุชุฑุญ ุฎุทูุฉ ุนูููุฉ ูููุณุชุฎุฏู (ูุซู: ุฐูุฑ ุชุงุฑูุฎ/ููุงู ุฃุฏูุ ุฃู ุฑุงุจุท ููุดูุฑุ ุฃู ูููุงุช ููุชุงุญูุฉ).
        4) ุงููุบุฉ: ุงูุนุฑุจูุฉ.
        """

    print(f"--- Sending prompt to LLM (Intent: {'Question' if is_question else 'Verification'}) ---")

    # --- LLM Invocation (Gemini Primary, Ollama fallback for local only) ---
    last_err = None
    
    # 1) Try Gemini first (required for cloud deployment)
    gemini_errors = []
    if GEMINI_API_KEY and genai is not None:
        try:
            genai.configure(api_key=GEMINI_API_KEY)
            # Use CORRECT model names for Gemini API v1beta
            # Reference: https://ai.google.dev/gemini-api/docs/models/gemini
            gemini_models = [
                os.getenv("GEMINI_MODEL"),  # Custom if provided
                "models/gemini-1.5-flash",
                "models/gemini-1.5-pro",
                "models/gemini-pro",
            ]
            
            for gemini_model in gemini_models:
                if not gemini_model:
                    continue
                try:
                    print(f"๐ Trying Gemini model: {gemini_model}")
                    model = genai.GenerativeModel(gemini_model)
                    resp = model.generate_content(
                        prompt,
                        generation_config={
                            "temperature": 0.1,
                            "top_p": 0.8,
                            "top_k": 40,
                            "max_output_tokens": 500,
                        }
                    )
                    content = getattr(resp, "text", None)
                    if not content and getattr(resp, "candidates", None):
                        parts = []
                        for c in resp.candidates:
                            for p in getattr(c, "content", {}).get("parts", []):
                                parts.append(str(p.get("text", "")))
                        content = "\n".join(parts)
                    if content:
                        print(f"โ Response generated successfully via Gemini ({gemini_model})")
                        return content.strip(), is_question
                    else:
                        err_msg = f"Empty response from {gemini_model}"
                        print(f"โ {err_msg}")
                        gemini_errors.append(err_msg)
                except Exception as model_err:
                    err_msg = f"{gemini_model}: {str(model_err)[:150]}"
                    print(f"โ Gemini model failed: {err_msg}")
                    gemini_errors.append(err_msg)
                    continue
            
            last_err = f"All Gemini models failed:\n" + "\n".join(f"  - {e}" for e in gemini_errors[-3:])
        except Exception as e:
            last_err = f"Gemini configuration error: {e}"
            print(f"โ Gemini failed: {e}")
    else:
        last_err = "Gemini API key not configured"
        print("โ Gemini not available (no API key)")

    # 2) Fallback to Ollama (local development only)
    try:
        forced = os.getenv("OLLAMA_MODEL")
        preferred_models = [
            forced if forced else "deepseek-v3.1:671b-cloud",
            "gpt-oss:120b-cloud",
            "llama3.2:1b",
            "phi3:mini",
        ]

        for m in preferred_models:
            try:
                resp = ollama.chat(
                    model=m,
                    messages=[{"role": "user", "content": prompt}],
                    options={"temperature": 0.1, "num_predict": 300},
                )
                print(f"โ Response generated via Ollama ({m})")
                return resp["message"]["content"], is_question
            except Exception as e:
                continue
    except Exception:
        pass

    # 3) If both failed, return clear error message
    error_msg = f"""โ๏ธ ุฎุทุฃ ูู ุงููุธุงู

ูู ูุชููู ุงููุธุงู ูู ุงูุงุชุตุงู ุจุฎุฏูุฉ ุงูุฐูุงุก ุงูุงุตุทูุงุนู.

ุงูุณุจุจ ุงููุญุชูู: {last_err}

ุงูุญู:
- ุชุฃูุฏ ูู ุฅุถุงูุฉ GEMINI_API_KEY ูู Secrets (Streamlit Cloud)
- ุฃู ุชุฃูุฏ ูู ูุฌูุฏ ุงูููุชุงุญ ูู ููู .env ูุญููุงู

ููููู ุงูุญุตูู ุนูู ููุชุงุญ ูุฌุงูู ูู: https://makersuite.google.com/app/apikey"""
    
    return error_msg, is_question
